<script type="text/javascript" src="https://cdn.bootcss.com/mathjax/2.7.2-beta.1/latest.js?config=default"></script>

<要改的太多了> typora打开，数学公式正常
[文章地址](http://www.zhaokv.com/wiki/book/ila/review)

=====线性代数总结（Review on Linear Algebra）=====
学习完Gilbert Strang的Introduction to Linear Algebra（Fourth Edition）受益匪浅，在这里进行摘要性总结。

通过总结不但可以对线性代数有一个总体上的认知，还方便日后查询，并且会不定期进行补充。

====1. 向量====
===1.1 向量和线性组合===
**关键词**：向量（Vector）、线性组合（Linear Combination）

向量通常是一列数字，为了书写方便一般表示为用圆括号括起来的一行数字，如二维向量${\bf v}=\left[\begin{matrix}&v_1\\&v_2\end{matrix}\right]$可以表示为$(v_1, v_2)$；

向量相加是向量中对应元素相加，如对于向量${\bf v}=(v_1,v_2)$和${\bf w}=(w_1,w_2)$，${\bf v}+{\bf w}=(v_1+w_1,v_2+w_2)$；

向量数乘是对向量中每个元素进行数乘，如对于向量${\bf v}=(v_1,v_2)$，$c{\bf v}=(cv_1,cv_2)$；

$c{\bf v}+d{\bf w}$是${\bf v}$和${\bf w}$的线性组合。

===1.2 长度和点积===
**关键词**：点积（Dot）、内积（Inner）

点积（也称为内积）${\bf v}\cdot{\bf w}$为$\sum v_iw_i$，显然${\bf v}\cdot{\bf w}={\bf w}\cdot{\bf v}$；

$||{\bf v}||$是向量${\bf v}$的长度，$||{\bf v}||=\sqrt{{\bf v} \cdot {\bf v}}$，单位向量${\bf u}$长度为1，${\bf u}={\bf v}/||{\bf v}||$是与${\bf v}$同方向的单位向量；

如果${\bf v}$和${\bf w}$是非零向量，则有$\frac{{\bf v}\cdot {\bf w}}{||{\bf v}|| ||{\bf w}||}=\cos\theta$，根据它我们能得到两个重要不等式$|{\bf v}\cdot {\bf w}|\le ||{\bf v}|| ||{\bf w}||$和$||{\bf v}+{\bf w}||\le||{\bf v}||+||{\bf w}||$。

===1.3 矩阵===
**关键词**：矩阵（Matrix）

多个列向量从左到右排列在一起组成一个矩阵，下面是一个$2\times 3$（2行3列）的矩阵。

$ {\bf A}=\left[\begin{matrix}1&& 2&& 3 \\ 4&& 5&& 6 \end{matrix}\right ]$

====2. 解线性方程组====
===2.1 矩阵运算法则===
**关键词**：交换律（Commutative Law）、分配律（Distribution Law）、结合律（Associative Law）、分块矩阵（Block Matrix）

对于矩阵${\rm A}$和矩阵${\rm B}$，${\rm A+B}$是矩阵对应元素相加，两个大小相同的矩阵才具有可加性；

如果${\rm A}$有$n$列且${\rm B}$有$n$行，${\rm A}$和${\rm B}$可以相乘得到${\rm AB}$；

在${\rm AB=C}$中：
* ${\rm C}$中第$i$行的第$j$个元素是$\text{(row i of A)}\cdot \text{(column j of B)}$
* ${\rm C}$的第$i$行是${\rm B}$列的线性组合，组合系数是${\rm A}$的第$i$行
* ${\rm C}$的第$j$列是${\rm A}$列的线性组合，组合系数是${\rm B}$的第$j$行
* ${\rm C}$还等于这些矩阵的和：$\sum\limits_{j} \text{(column i of A)}\cdot \text{(row j of B)} $；

一个$m\times n$的矩阵与一个$n\times p$的矩阵相乘所需的乘法次数是$mnp$次（因为乘法相对于加法更加耗时，所以我们竭尽所能减少乘法次数）；

一连串矩阵相乘时可以利用动态规划来实现优化；

矩阵加法满足${\rm A+B=B+A}$（交换律），$c{\rm(A+B)}=c{\rm A}+c{\rm B}$（数乘的分配律），${\rm A+(B+C)=(A+B)+C}$（结合律）；

矩阵乘法满足${\rm C(A+B)}={\rm CA}+{\rm CB}$（左分配律），${\rm (A+B)C}={\rm AC}+{\rm BC}$（右分配律），${\rm A(BC)=(AB)C}$（结合律）；

注意矩阵乘法不满足交换律，也就是说一般情况下${\rm AB \neq BA}$，但单位矩阵${\rm I}$以及$c{\rm I}$与任何矩阵都能满足交换律；

当矩阵${\rm A}$是方阵时，${\rm A}^p={\rm AAA\cdots A}$（$p$个相乘），$({\rm A}^p)({\rm A}^q)={\rm A}^{p+q}$，$({\rm A}^p)^q={\rm A}^{pq}$；

如果A和B中的分块是可以相乘的，则有
$\left[\begin{matrix}{\rm A}_{11} && {\rm A}_{12} \\{\rm A}_{21} && {\rm A}_{22} \\\end{matrix}\right]$$\left[\begin{matrix}{\rm B}_{11} && {\cdots} \\{\rm B}_{21} && {\cdots} \\\end{matrix}\right]=$$\left[\begin{matrix}{\rm A}_{11}{\rm B}_{11}+{\rm A}_{12}{\rm B}_{21} && {\cdots} \\{\rm A}_{21}{\rm B}_{11}+{\rm A}_{22}{\rm B}_{21} && {\cdots} \\\end{matrix}\right]$。

===2.2 逆矩阵===
**关键词**：逆矩阵（Inverse matrix）、可逆（Inversible）、奇异（Singular）、高斯-乔丹（Gauss-Jordan）、消元（Elimination）、主元（Pivot）、三对角（Tridiagonal）、稠密矩阵（Dense Matrix）

矩阵${\rm A}$可逆当且仅当存在矩阵${\rm A^{-1}}$，使得${\rm A^{-1}A=I}$和${\rm AA^{-1}=I}$成立，其中${\rm A^{-1}}$称为${\rm A}$的逆矩阵，逆矩阵有逆操作的作用，如果${\rm A}$不可逆，称${\rm A}$为奇异矩阵；

关于逆矩阵有以下几点值得注意：
* 但在大多数问题中我们几乎不计算逆矩阵
* 逆矩阵存在当且仅当消元后有$n$个主元
* 一个矩阵只有一个逆矩阵，且左逆和右逆相同
* 如果${\rm A}$可逆，则${\rm A}{\bf x}={\bf b}$的唯一解为${\bf x}={\rm A}^{-1}{\bf b}$
* 假设${\rm A}{\bf x}={\bf 0}$有非零解，则${\rm A}$没有逆矩阵
* 如果一个对角矩阵的对角上没有零，则它可逆，它的逆为对角线上元素去倒数

如果${\rm A}$和${\rm B}$均是可逆的，则${\rm (AB)}^{-1}={\rm B}^{-1}{\rm A}^{-1}$，注意${\rm A+B}$的逆就没有什么结论；

可以通过高斯-乔丹消元法计算${\rm A}^{-1}$，根据$[{\rm A}\quad {\rm I}]$乘${\rm A}^{-1}$得到$[{\rm I}\quad {\rm A}^{-1}]$，我们可以通过对增广矩阵$[{\rm A}\quad {\rm I}]$进行行消元得到${\rm A}^{-1}$；

如果矩阵${\rm K}$关于主对角线对称，则${\rm K}^{-1}$也关于主对角线对称，如果${\rm K}$是三对角矩阵，则${\rm K}^{-1}$是稠密矩阵。

===2.3 转置和置换===
**关键词**：转置（Transpose）、对称矩阵（Symmetric Matrix）、置换矩阵（Permutation Matrix）

转置是把一个矩阵行与列进行互换，如对于矩阵${\rm A}$转置为$({\rm A^T})_{ij}={\rm A}_{ji}$；

对于矩阵${\rm A}$和${\rm B}$，${\rm (A+B)^T}={\rm A^T}+{\rm B^T}$，${\rm (AB)^T}={\rm B^T}{\rm A^T}$，${\rm (A^{-1})^T}=({\rm A^T})^{-1}$；

如果对于矩阵${\rm A}$有$a_{ij}=a_{ji}$，即${rm A^T=A}$，则称${\rm A}$为对称矩阵；

对于任意矩阵${\rm R}$，${\rm R^TR}$和${\rm RR^T}$都是对称矩阵且它们对角线上元素非负，大多数科学问题都是从一个矩阵${\rm R}$开始以${\rm R^TR}$或${\rm RR^T}$结束；

对称矩阵${\rm A}$可分解为${\rm A=LDL^T}$；

置换矩阵${\rm P}$的行向量是单位矩阵${\rm I}$的行向量，这些行向量可以以任意顺序重排，并且有$P^T=P^{-1}$；

对于任意可逆矩阵${\rm A}$，存在一个置换矩阵矩阵${\rm P}$重排${\rm A}$的行后可以进行LU分解，即${\rm PA=LU}$；

对于非零矩阵${\rm A}$，${\rm A^2}$可以是零矩阵，但${\rm A^TA}$不可能是零矩阵。

===2.4 线性方程组===
**关键词**：线性方程组（Linear Equations）、系数矩阵（Coefficient Matrix）、上三角（Upper Triangular）、下三角（Lower Triangular）、增广矩阵（Augmented Matrix）、置换（Permutation）

求解线性方程组是线性代数的核心问题；

${\rm A}{\bf x}$是${\rm A}$列向量的线性组合，求解${\rm A}{\bf x}={\bf b}$是寻找一个特定组合来产生${\bf b}$，其中${\rm A}$被称为系数矩阵；

从另外一个角度来看是寻找多条线（平面或超平面）的交点；

每个超级计算机都会拿求解${\rm A}{\bf x}={\bf b}$来测试速度；

用计算机求解线性方程组并没有什么高深的方法，用的正是消元法，当然需要很多技巧来保证最后解的质量；

消元过程中线（平面或超平面）的位置会变化，但它们的交点不会变；

线性系统${\rm A}{\bf x}={\bf b}$消元之后变成上三角${\rm U}{\bf x}={\bf b}$或下三角；

$[{\rm A} \quad {\bf b}]$被称为增广矩阵，左乘消元矩阵实现消元，如$[{\rm E_{21}}{\rm A} \quad {\rm E_{21}}{\bf b}]$，左乘置换矩阵实现行置换；

计算机编程中应该直接进行消元或者置换，而不应该采用矩阵相乘来实现，因为矩阵相乘的方式带来更多冗余计算。

===2.5 LU分解===
**关键词**：分解（Factorization）、带状矩阵（Band Matrix）

如果${\rm A}$是$3\times 3$的矩阵，当在不进行行交换的前提下有${\rm (E_{32}E_{31}E_{21})A=U}$时，则有${\rm A=(E_{21}^{-1}E_{31}^{-1}E_{32}^{-1})U}$，即${\rm A=LU}$，其中${\rm L}$是下三角矩阵，${\rm U}$是上三角矩阵；

更一般的情况是不存在行交换对${\rm A}$进行高斯消元可以把${\rm A}$分解为${\rm L}$和${\rm U}$，${\rm U}$是消元后产生的上三角矩阵，${\rm L}$包含消元过程中的乘子$l_{ij}$；

如果把${\rm U}$的对角线元素单独放在对角矩阵${\rm D}$中且把${\rm U}$对角元素设为1，则可以把${\rm A=LU}$写成${\rm A=LDU}$；

存储矩阵LU分解后的结果和存储原矩阵需要相同的空间，但是LU分解能更快地求解线性方程组，尤其是在线性系统${\rm A}{\bf x}={\bf b}$中${\rm A}$不变但${\bf b}$不断变化的情况下；

对于${\rm A}{\bf x}={\bf b}$，当有${\rm A=LU}$时，${\rm L}{\bf c}={\bf b}$进而${\rm U}{\bf x}={\bf c}$；

如果${\rm A}$是$n\times n$的，则LU分解分别需要$\frac{1}{3}n^3$个乘法和减法操作，求解时对右侧分别需要$n^2$个乘法和减法操作；

如果${\rm A}$是带状矩阵（只有对角线上下有元素），设$w$是带宽，则$\frac{1}{3}n^3 $变成$nw^2$，$n^2$变成$2nw$。

====3. 向量空间和子空间====
===3.1 向量空间===
**关键词**：子空间（Subspace）、列空间（Column Space）

空间${\rm R^n}$包含任何含有$n$个实数元素的列向量${\bf v}$，我们可以把${\rm R^n}$中任意向量加在一起，也可以为任何向量乘上一个数字$c$，得到的结果都在这个空间中；

对于一个集合，如果集合中元素对加法和数乘是封闭的，那么这个集合构成一个空间；

在众多空间中有一个空间最为特殊，那就是空间${\rm Z}$，它只包含一个零向量；

一个向量空间的子空间满足如下条件：如果${\bf v}$和${\bf w}$都在这个子空间中，则${\bf v+w}$与$c{\bf v}$也在这个子空间中，更进一步可知$c{\bf v}+d{\bf w}$也在子空间中；

矩阵${\rm A}$列向量的线性组合构成它的列空间，一般记为$C({\rm A})$，当${\bf b}$在$C({\rm A})$时${\rm A}{\bf x}={\bf b}$有解。

===3.2 ${\rm A}$的NULL空间与秩===
**关键词**：NULL空间（Nullspace）、行阶梯矩阵（Reduced Row Echelon Form）、秩（Rank）

${\rm A}$的NULL空间包括满足${\rm A}{\bf x}={\bf 0}$所有的${\rm x}$，记为$N({\rm A})$；

消元产生阶梯矩阵${\rm U}$，进一步可以得到行阶梯矩阵${\rm R}$，其中有主元列和自由列；

对于${\rm R}$令不同自由列分别等于1通过回代可以得到${\rm A}{\bf x}={\bf 0}$若干特解，这些特解的线性组合构成${\rm A}{\bf x}={\bf 0}$的所有解，也就是$N({\rm A})$；

根据定义可知一个矩阵的行空间和它的NULL空间相互正交；

矩阵的秩是矩阵主元的数量，可以记为$r$，它不但是行向量的维度还是列向量的维度，而且NULL空间的维度是列空间的维度减去$r$；

对${\rm A}{\bf x}={\bf 0}$ 中的${\rm A}$进行消元变换得到行阶梯矩阵${\rm R}$后，如果${\rm R}=\left[\begin{matrix} I&& F \\ 0&& 0 \end{matrix}\right]$ ，则NULL空间矩阵可以表示为${\rm N}=\left[\begin{matrix}  -F \\ 0 \end{matrix}\right]$；

对于矩阵${\rm A}$和${\rm B}$，存在结论$\text{rank}({\rm AB})\le \min(\text{rank}({\rm A}), \text{rank}({\rm B}))$。

===3.3 ${\rm A}{\bf x}={\bf b}$全部解===
**关键词**：特解（Special Solution）

求解${\rm A}{\bf x}={\bf b}$可以看做求以${\rm A}$列向量为坐标轴时${\bf b}$的坐标值${\rm x}$；

对增广矩阵进行行变换时相当于在变换坐标轴，但坐标值并没有发生变化；

${\rm A}{\bf x}={\bf b}$可解当且仅当最后$m-r$个等式变成$0=0$；

${\rm A}{\bf x}={\bf b}$的所有解是满足${\rm A}{\bf x}={\bf b}$的一个特解${\bf x}_p$加上${\rm A}{\bf x}={\bf 0}$的解；

求特解${\bf x}_p$时可以把所有自由向量设为0，一旦自由向量选定之后特解也就确定了；

根据秩$r$的取值线性方程组有如下四种可能：

* ${r=m,r=n}$，方阵且可逆，${\rm A}{\bf x}={\bf b}$有且只有一个解
* ${r=m,r < n} $，矮又宽型，${\rm A}{\bf x}={\bf b}$有无穷多解
* ${r< m,r=n}$，高又瘦型，${\rm A}{\bf x}={\bf b}$有一个解或无解
* ${r< m,r< n}$，行或列都不满秩，${\rm A}{\bf x}={\bf b}$有无穷多解或无解

===3.4 线性无关、基和维度===
**关键词**：线性无关（Independence）、基（Basis）、维度（Dimension）、张成（Span）

向量${\bf v_1,\cdots,v_n}$当且仅当$x_1{\bf v_1}+ x_2{\bf v_2}+\cdots+ x_n{\bf v_n}={\bf 0}$中的$x$全为零，可见${\rm A}$的列线性无关等价于${\rm A}{\bf x}={\bf 0}$中${\bf x}={\bf 0}$；

一组向量的线性组合所构成的空间是这组向量所张成的空间，空间中线性无关且能张成这一空间的一组向量可以作为这一空间的基；

所有基包含向量个数相同，我们把这个数称为该空间的维度，矩阵${\rm A}$列向量所张成空间的维度等于矩阵的秩；

空间中每个向量表示为一组基线性组合是唯一的；

空间${\rm Z}$只包含零向量，这一空间的维度为0，空集构成此空间的一组基；

对于空间${\rm V}$和${\rm W}$有$\text{dim}({\rm V})+ \text{dim}({\rm W})= \text{dim}({\rm V \bigcap W})+ \text{dim}({\rm V \bigcup W})$。
===3.5 四个子空间的维度===
**关键词**：左NULL空间（Left Nullspace）

对于$m\times n$的实数矩阵${\rm A}$有如下四个基础子空间：
* 行空间${C({\rm A^T})}$，是${\rm R^n}$的子空间，维度等于秩$r$
* 列空间${C({\rm A})}$，是${\rm R^m}$的子空间，维度等于秩$r$
* NULL空间${N({\rm A})}$，是${\rm R^n}$的子空间，维度等于$n-r$
* 左NULL空间${N({\rm A^T})}$，是${\rm R^m}$的子空间，维度等于秩$m-r$

对矩阵${\rm A}$进行消元得到${\rm R}$后，注意$C({\rm A})\neq C({\rm R})$，但是$C({\rm A^T})= C({\rm R^T})、N({\rm A})= N({\rm R}), N({\rm A^T})= N({\rm R^T})$；

每个秩为1的矩阵都可以分解为由两个向量相乘的特殊形式，${\rm A}={\bf u}{\bf v^T}$。

====4 正交====
===4.1 四个子空间的正交性===
**关键词**：正交（Orthogonality）、正交补（Orthogonal Complement）

对于两个向量${\bf v}$和${\bf w}$，两者内积等于零${\bf v}^{\rm T}{\bf w}=0$时称为两个向量正交，这时有$||{\bf v}||^2+||{\bf w}||^2=||{\bf v}+{\bf w}||^2$；

对于两个向量子空间${\rm V}$和${\rm W}$，如果${\rm V}$中任意向量${\bf v}$与${\rm W}$中任意向量${\bf w}$都正交${\bf v}^{\rm T}{\bf w}=0$，这两个向量子空间也正交；

$m\times n$矩阵${\rm A}$的NULL空间$N({\rm A})$与行空间$C({\rm A^T})$是${\rm R}^n$中的正交子空间，左NULL空间$N({\rm A^T})$与列空间$C({\rm A})$是${\rm R}^m$中的正交子空间；

如果子空间${\rm V^{\bot}}$包含所有与子空间${\rm V}$正交的向量，则${\rm V^{\bot}}$称为子空间${\rm V}$的正交补，所以上面两对子空间不但正交，而且是两对正交补；

${\rm R}^n$中任意$n$个线性无关向量张成${\rm R}^n$空间，同时能张成${\rm R}^n$空间的$n$个向量线性无关，这些向量构成一组基；

${\rm R}^n$中的每个${\bf x}$都有一个NULL空间组成部分${\bf x}_n$和行空间组成部分${\bf x}_r$$，{\rm R}^m$中的每个${\bf x}$都有一个左NULL空间组成部分${\bf x}_n$和列空间组成部分${\bf x}_r$。

===4.2 投影===
**关键词**：投影（Projection）

把${\bf b}$投影到通过${\bf a}$的直线上时，误差为${\bf e=b}-\hat{x}{\bf a}$，根据${\bf a\cdot e}={\bf a\cdot(b-}\hat{x}{\bf a})=0$得到$\hat{x}=\frac{{\bf a}\cdot{\bf b}}{{\bf a}\cdot{\bf a}}= \frac{{\bf a}^{\rm T}{\bf b}}{{\bf a}^{\rm T}{\bf a}}$，则投影为${\bf p}=\hat{x}{\bf a}= {\bf a}\frac{{\bf a}^{\rm T}{\bf b}}{{\bf a}^{\rm T}{\bf a}}$；



把${\bf b}$投影到通过${\bf a}$的直线上的投影矩阵为${\rm P}=\frac{{\bf a}{\bf a}^{\rm T}}{{\bf a}^{\rm T}{\bf a}}$；

假设${\rm R}^m$中的$n$个向量${\bf a}_1,\cdots, {\bf a}_n$线性无关，对于给定的任意${\bf b}$离它最近的线性组合${\bf p}=\hat{x}{\bf a}_1+\cdots +\hat{x}{\bf a}_n={\rm A}\hat{{\rm x}}$是${\bf b}$在${\rm R}^m$中的投影；

由${\bf b}-{\rm A}\hat{{\bf x}}$与${\bf a}_1,\cdots, {\bf a}_n$均垂直得到${\rm A^T}({\bf b}-{\rm A}\hat{{\bf x}})={\bf 0}$，整理可得
${\rm A^TA}\hat{{\bf x}}={\rm A^T}{\bf b}$，根据上面假设有$\hat{{\bf x}}=({\rm A^TA})^{-1}{\rm A^T}{\bf b}$；

${\bf p}={\rm A}\hat{{\bf x}}={\rm A}({\rm A^TA})^{-1}{\rm A^T}{\bf b}$，根据${\bf p}={\rm P}{\bf b}$得${\rm P}=({\rm A^TA})^{-1}{\rm A^T}$；

如果${\rm P}$是投影矩阵，则${\rm P}^2={\rm P}$，因为第二次投影不会做任何事，另外${\rm P^T=P}$说明${\rm P}$是对称阵，${\rm I-P}$也是投影矩阵，因为$({\rm I-P}){\bf b=b-p=e}$，也就是说${\rm P}$投影到一个子空间，${\rm I-P}$投影到与之垂直的另外一个子空间上；

如果${\rm A}$列线性无关则${\rm A^TA}$是对称可逆方阵，如果${\rm A}$行线性无关则${\rm AA^T}$是对称可逆方阵，而对于可逆更一般的结论是如果${\rm A}$有$r$线性无关的列且${\rm B}$有$r$线性无关的行则${\rm AB}$可逆。

===4.3 最小二乘法===
**关键词**：最小二乘（Least Square）

${\rm A}{\bf x}={\bf b}$经常无解，一般是因为有太多方程，$m\times n$矩阵${\rm A}$的行要比列多（$m>n$），$n$个列向量只能张成$m$维空间的一小部分，${\bf b}$很可能在这个空间之外；

我们不是总能令${\bf e}={\bf b}-{\rm A}{\bf x}$等于零，当${\bf e}$等于零时${\bf x}$是${\rm A}{\bf x}={\bf b}$的精确解，${\bf e}$不等于零但${\bf e}$取最小时，$\hat{{\bf x}}$是最小二乘解；

每一个${\bf b}$都可以分为两部分，在列空间中的${\bf p}$和左NULL空间中的${\bf e}$，则有$||{\rm A}{\bf x}-{\bf b}||^2=||{\rm A}{\bf x}-{\bf p}||^2+||{\bf e}||^2$，当${\rm A}{\bf x}-{\bf p}={\bf 0}$时$||{\rm A}{\bf x}-{\bf b}||^2$最小；

当${\rm A}{\bf x}={\bf b}$无解时，根据投影乘以${\rm A^T}$然后求解${\rm A^TA}\hat{{\bf x}}={\rm A^T}{\bf b}$得到最小二乘解，它有最小平均平方误差（MSE），对$||{\rm A}{\bf x}-{\bf b}||^2$求导并令其等于零也能得到同样的结果；

上述过程一般是在${\rm A}$的列向量线性无关的情况下来讨论的，用来保证${\rm A^TA}$可逆，不可逆时有多个解。

===4.4 正交基和Gram-Schmidt===
**关键词**：正交基（Orthogonal Base）、正交矩阵（Orthogonal Matrix）、QR分解（QR Factorization）

如果矩阵${\rm Q}$的列向量${\bf q}_1,\cdots, {\bf q}_n$满足${\bf q}_i^{\rm T}{\bf q}_j=0$且${\bf q}_i^{\rm T}{\bf q}_i=1$，即${\rm Q^T}{\rm Q}={\rm I}$，则它们标准正交，当${\rm Q}$是方阵时有${\rm Q^T}={\rm Q^{-1}}$，这种情况下称${\rm Q}$为正交矩阵；

旋转矩阵、置换矩阵和反射矩阵（如果反射平面的法线是${\rm u}$，则反射矩阵为${\rm I}-2{\bf u}{\bf u}^T$）都是正交矩阵；

如果矩阵${\rm Q}$列标准正交${\rm Q^T}{\rm Q}={\rm I}$，则对任何${\bf x}$都有$||{\rm Q}{\bf x}||=||{\bf x}||$；

正交矩阵在计算中表现非常出色，因为当向量长度标准化后数值永远不会太大，所以稳定的代码会尽可能多使用${\rm Q}$；

投影到由矩阵${\rm Q}$列向量${\bf q}_1,\cdots, {\bf q}_n$所张成空间的投影矩阵是${\rm P=QQ^T}$；

对于任意向量${\bf b}$，它的投影为${\bf p}={\bf q}_1({\bf q}_1^{\rm T}{\bf b})+\cdot+ {\bf q}_n({\bf q}_n^{\rm T}{\bf b})$，如果${\rm Q}$是方阵，则有${\bf b}={\bf q}_1({\bf q}_1^{\rm T}{\bf b})+\cdot+ {\bf q}_n({\bf q}_n^{\rm T}{\bf b})$；

从任意三个线性无关的向量${\bf a},{\bf b},{\bf c}$开始，我们想要构造三个正交向量${\bf A},{\bf B},{\bf C}$，进而得到三个标准正交向量${\bf q}_1={\bf A}/||{\bf A}||,{\bf q}_2={\bf B}/||{\bf B}||,{\bf q}_3={\bf C}/||{\bf C}||$：
* ${\bf B}={\bf b}-\frac{{\bf A}^{\rm T}{\bf b}}{{\bf A}^{\rm T}{\bf A}}{\bf A}$
* ${\bf C}={\bf c}-\frac{{\bf A}^{\rm T}{\bf c}}{{\bf A}^{\rm T}{\bf A}}{\bf A}-\frac{{\bf B}^{\rm T}{\bf c}}{{\bf B}^{\rm T}{\bf B}}{\bf B}$

$\left[\begin{align} && \\ {\bf a}&&{\bf b}&&{\bf c}\\ && \\ \end{align}\right]=$  $\left[\begin{align} && \\ {\bf q}_1&&{\bf q}_2&&{\bf q}_3\\ && \\ \end{align}\right]$  $\left[\begin{align} {\bf q}_1^{\rm T}{\bf a}&& {\bf q}_1^{\rm T}{\bf b}&&{\bf q}_1^{\rm T}{\bf c} \\                         &&{\bf q}_2^{\rm T}{\bf b}&&{\bf q}_2^{\rm T}{\bf c}\\ && &&{\bf q}_3^{\rm T}{\bf c}\\ \end{align}\right]$，即${\rm A}={\rm Q}{\rm R}$，乘${\rm Q^T}$得到${\rm R}={\rm Q^T}{\rm A}$；

${\rm A^T}{\rm A}$等价于${\rm R^TQ^TQR=R^TR}$，则求解最小二乘${\rm A^TA}\hat{\bf x}={\rm A^T}{\bf b}$简化为${\rm R}{\bf x}={\rm Q^T}{\bf b}$或$\hat{\bf x}={\rm R^{-1}Q^T}{\rm b}$；

求解${\rm A}{\bf x}={\bf b}$几乎是不可能的，我们取而代之通过回代求解${\rm A^TA}\hat{\bf x}={\rm A^T}{\bf b}$，实际消耗是Gram-Schmidt时$mn^2$次乘法；

当然类似matlab等成熟软件并不使用Gram-Schmidt，而是使用更好的Householder reflection方法来产生${\rm R}$。

====5 行列式====
===5.1 行列式的性质===
**关键词**：行列式（Determinant）

方阵${\rm A}$主元的乘积称为这个方阵的行列式，它包含大量关于${\rm A}$的信息，一般记为$\det {\rm A}$或$|{\rm A}|$；

关于行列式存在十个性质：
- $n\times n$单位阵的行列式为1
- 当两行交换时行列式符号会改变
- 行列式是分别关于每一行的线性函数
- 如果矩阵中两行相等则行列式等于0
- 从一行中减去另外一行的若干倍整个行列式不变
- 如果矩阵中有一行都等于零则行列式等于0
- 如果矩阵是对角矩阵则行列式等于对角元素的乘积
- 如果矩阵奇异则行列式等于0，非奇异时行列式不为0
- ${\rm |AB|=|A||B|}$
- ${\rm |A^T|=|A|}$ 

行列式是分别关于每一行的线性函数举例说明：在其他行不变的情况下，当第一行数乘$t$时行列式数乘$t$，当第一行相加是行列式相加。

===5.2 组合与代数余子式===
**关键词**：代数余子式（Cofactor）

矩阵${\rm A}$的左上角$k\times k$矩阵记为${\rm A}_k$，令$d_k$表示${\rm A}$第$k$个主元，则有$d_k=\frac{d_1d_2\cdots d_k}{d_1d_2\cdots d_{k-1}}=\frac{|{\rm A}_k|}{|{\rm A}_{k-1}|}$；

关于全部（$n!$个）置换矩阵${\rm P}$求和得到${\rm A}$的行列式：
$|A|=\sum|{\rm P}|a_{1\alpha}a_{2\beta}\cdots a_{n\omega}$；

矩阵${\rm A}$的去掉第$i$行和第$j$列后所形成$n-1$阶矩阵记为${\rm M}_{ij}$，则${\rm A}$的代数余子式${\rm C}_{ij}$为：
${\rm C}_{ij}=(-1)^{i+j}|{\rm M}_{ij}|$；

代数余子式可以递归计算行列式的值：$|{\rm A}|=a_{i1} {\rm C}_{i1}+ a_{i2} {\rm C}_{i2}+\cdots+ a_{in} {\rm C}_{in}$；

上面两种行列式的计算方式适合分析而不适合计算，主元更适合计算行列式等。

===5.3 克莱姆法则、逆与体积===
**关键词**：克莱姆法则（Cramer's Rule）、叉积（Cross Product）

如果${\rm |A|}$不为零，${\rm A}{\bf x}={\bf b}$可以通过特征值求解：
$x_1=\frac{{\rm |B_1|}}{{\rm |A|}}$    $x_2=\frac{{\rm |B_2|}}{{\rm |A|}}$    $\cdots$    $x_n=\frac{{\rm |B_n|}}{{\rm |A|}}$，
其中矩阵${\rm B_j}$是通过将${\rm A}$的第$j$列替换为向量${\bf b}$得到的；

${\rm A^{-1}}=\frac{{\rm C^T}}{{\rm |A|}}$，其中${\rm C}$是${\rm A}$的代数余子式矩阵；

如果一个“盒子”以矩阵${\rm A}$的行为边，则${\rm |A|}$是立方体的体积；

${\bf u}=(u_1,u_2,u_3)$和${\bf v}=(v_1,v_2,v_3)$的叉积是一个与${\bf u}$和${\bf v}$相垂直的向量：

${\bf u}\times {\bf v}=\left|\begin{align} {\bf i}&& {\bf j}&& {\bf k}\\ u_1&& u_2&& u_3 \\ v_1&& v_2&& v_3 \end{align}\right|=(u_2v_3-u_3v_2){\bf i}+(u_3v_1-u_1v_3){\bf j}+(u_1v_2-u_2v_1){\bf k}$；

${\bf v}\times {\bf u}=-{\bf u}\times {\bf v}$，$||{\bf u}\times {\bf v}||=||{\bf u}||||{\bf v}|||\sin\theta|$；

三重积$({\bf u}\times {\bf v})\cdot{\bf w}= \left|\begin{align} w_1&& w_2&& w_3\\ u_1&& u_2&& u_3 \\ v_1&& v_2&& v_3 \end{align}\right|$$= \left|\begin{align} u_1&& u_2&& u_3\\ v_1&& v_2&& v_3 \\ w_1&& w_2&& w_3 \end{align}\right|$，它是以${\bf u}$、${\bf v}$和${\bf w}$为边所构成“盒子”的体积。

====6 特征值与特征向量====
===6.1 什么是特征值===
**关键词**：特征值（Eigenvalue）、特征向量（Eigenvector）、迹（Trace）、对角化（Diagonalize）

对于方阵${\rm A}$有一些向量${\bf x}$与之相乘后的${\rm A}{\bf x}$与${\bf x}$方向相同，这些向量便是${\rm A}$的特征向量，等式${\rm A}{\bf x}=\lambda{\bf x}$中的$\lambda$便是对应的特征值；

当对${\rm A}$平方时特征向量不会发生改变，而特征值会被平方，另外拥有特殊性质的矩阵会有特殊的特征值与特征向量；

我们可以通过特征多项式$\det({\rm A}-\lambda{\rm I})=0$求得特征值，再对每一个$\lambda$求解$({\rm A}-\lambda{\rm I}){\bf x}={\bf 0}$或${\rm A}{\bf x}=\lambda{\bf x}$得到特征向量${\bf x}$；

值得注意的是在计算机中求解特征值和特征向量并不是利用上述方式，因为计算机并不是特别擅长求解一元高次方程；

对一个矩阵进行消元或者行变换都会改变这个矩阵的特征值；

$n$个特征值的成绩等于矩阵的行列式的值，$n$个特征值的和等于矩阵的迹，矩阵的迹是矩阵对角线元素之和；

假设一个$n\times n$的矩阵${\rm A}$有$n$个线性无关的特征向量${\bf x}_1,\cdots,{\bf x}_n$，把它们放到特征向量矩阵${\rm S}$中，根据${\rm AS}={\rm S}{\Lambda}$则有：

${\rm S^{-1}}{\rm A}{\rm S}=\Lambda=\left[\begin{align} \lambda_1&& && \\  && \cdots &&  \\  &&  && \lambda_n \end{align}\right]$；

上面的过程成为对角化，在实际中从概率上来讲大部分概率都可以对角化的；

${\rm A}^k={\rm S}{\Lambda}^k{\rm S^{-1}}​$，可见${\rm A}^k​$特征向量不变，特征值为$\lambda_1^k,\cdots,\lambda_n^k​$；

存在递推关系${\bf u}_{k+1}={\rm A}{\bf u}_k$，从${\bf u}_0$开始$k$步后结果为${\bf u}_{k}={\rm A}^k{\bf u}_0={\rm S}{\Lambda}^k{\rm S}^{-1}{\bf u}_0$；

如果${\bf u}_0=c_1{\bf x}_1+\cdots+c_n{\bf x}_n$，则有${\bf u}_k=c_1(\lambda_1)^k{\bf x}_1+\cdots+c_n(\lambda_n)^k{\bf x}_n$，其中$c$来源于${\rm S}^{-1}{\bf u}_0$，${\lambda}^k$来源于${\Lambda}^k$，${\bf x}$来源于${\rm S}$；

对于${\rm AB}$和${\rm A+B}$，特征值与特征向量方面并没有什么特殊性质。
===6.2 微分方程===
**关键词**：微分方程组（Differential Equations）

利用特征值可以求解一类线性方程组，那就是方程${\bf u}'={\rm A}{\bf u}$关于常系数是线性的；

从${\bf u}(0)$开始，它的解为${\bf u}(t)=c_1e^{\lambda_1 t}{\bf x}_1+\cdots+ c_ne^{\lambda_n t}{\bf x}_n$，系数由${\bf u}(0)=c_1{\bf x}_1+\cdots+c_n{\bf x}_n={\rm S}{\bf c}$得到；

$e^{{\rm A}t}={\rm I}+{\rm A}t+\frac{1}{2}({\rm A}t)^2+\frac{1}{6}({\rm A}t)^3+\cdots$，则方程解还可以写为${\bf u}(t)=e^{{\rm A}t}{\bf u}(0)$；

有$y''$的方程通过把$y'$和$y$合并为${\bf u}=(y,y')$转换为${\bf u}'={\rm A}{\bf u}$。
===6.3 对称矩阵===
**关键词**：对称矩阵（Symmetric Matrix）、实矩阵（Real Matrix）、共轭（Conjugate）

对称矩阵是线性代数中（无论理论还是实践）最重要的矩阵，这么说一点都不过分；

实对称矩阵的特征向量一定可以选为相互正交（不同特征值对应的特征向量相互正交），而特征值一定是实数，同时特征值的符号与主元符号相符；

所有实对称矩阵都可以对角化，对角化变为${\rm A=Q\Lambda Q^T}$，其中${\rm Q}$是正交矩阵；

如果${\bf x}_1,\cdots,{\bf x}_n$是${\rm A}$的列向量，则有${\rm A}=\lambda_1{\rm P}_1+\cdots+\lambda_n{\rm P}_n$，其中$\lambda_i$是特征值，${\rm P}_i={\bf x}_i{\bf x^T}_n$是投影到相应特征向量上的投影矩阵；

对于实矩阵，复数$\lambda$与${\bf x}$是共轭成对出现的，也就是说如果有${\rm A}{\bf x}=\lambda{\bf x}$，则有${\rm A}\bar{{\bf x}}=\bar{\lambda}\bar{{\bf x}}$；

每一个方阵都可以分解为${\rm A=QTQ^{-1}}$的形式，其中${\rm T}$是上三角矩阵且$\bar{{\rm Q}}^{\rm T}$，如果${\rm A}$的特征值是实数则存在实数矩阵${\rm Q}$和${\rm T}$；

如果$\lambda_{max}$是实对称矩阵${\rm A}$最大的特征值，则对角线上元素不可能有比它大的。
===6.4 正定矩阵===
**关键词**：正定矩阵（Positive Definite Matrix）

对任何非零向量${\bf x}$都有${\bf x}^{\rm T}{\rm A}{\bf x}>0$时，我们称${\rm A}$为正定矩阵；

矩阵${\rm A}$正定当且仅当它的特征值或主元都为正数，利用主元要比特征值快好多，另外正定矩阵的对角线上不会出现零；

根据上一条可以推出${\rm A}$正定当且仅当${\rm A}$的$n$个左上角矩阵行列式大于零；

如果${\rm A}$与${\rm B}$都对称正定，则${\rm A+B}$也对称正定；

如果${\rm R}$的列向量线性无关，则${\rm A=R^TR}$不但是对称矩阵，而且正定；

椭圆${\bf x}^{\rm T}{\rm A}{\bf x}=1$的轴沿着${\rm A}$的特征向量方向，轴长为$1/\sqrt{\lambda}$。

===6.5 相似矩阵===
**关键词**：相似矩阵（Similar Matrix）

如果存在任一矩阵${\rm M}$使得${\rm B=M^{-1}AM}$，则${\rm B}$与${\rm A}$相似，从定义上就可以看出相似是可逆的；

没有被${\rm M}$改变的有：特征值、迹、行列式、秩和无关特征向量个数，被改变的有：特征向量、NULL空间、列空间、行空间、左NULL空间和奇异值；

如果${\rm A}$有$n$个线性无关的特征向量，则${\rm A}$与$\Lambda$相似且${\rm M=S}$。

===6.6 奇异值分解===
**关键词**：奇异值分解（Singular Value Decomposition）

奇异值分解在线性代数中非常重要，它可以对角化任意矩阵，有很多实际用途，如降维等；

把秩为$r$的${\rm A}$通过SVD分解为${\rm U\Sigma V^T}$，$\Sigma$对角线上是奇异值$\sigma_1\ge\cdots\sigma_r>0$：
* $\sigma_1^2,\cdots,\sigma_r^2$是${\rm AA^T}$或${\rm A^TA}$特征值（两者特征值相同）
* ${\rm U}$是${\rm AA^T}$的特征向量，前$r$列来源于${\rm A}$的列空间，后$m-r$列来源于$N({\rm A^T})$
* ${\rm V}$是${\rm A^TA}$的特征向量，$r$列是${\rm A}$的行空间，后$n-r$列来源于$N({\rm A})$

${\Sigma}$对角线上奇异值从小到大排列，奇异值越大对整个矩阵的重要性也越大；

${\rm U}$和${\rm V}$的列向量分别是输出空间和输入空间的坐标轴。
====7 线性变换====
===7.1 什么是线性变换===
**关键词**：线性变换（Linear Transformation）、范围（Range）、核（Kernel）

变换$T$以空间${\rm V}$中任一向量${\bf v}$为输入，以$T({\bf v})$为输出，如果对任意${\bf v}$与${\bf w}$都满足如下条件，则此变换是线性的：
* $T({\bf v}+{\bf w})=T({\bf v})+T({\bf w})$
* 对所有$c$有$T(c{\bf v})=cT({\bf v})$
* 可以合并为对所有$c$与$d$有$T(c{\bf v}+d{\bf w})=cT({\bf v})+dT({\bf w})$

变换$T$的范围是所有输出$T({\bf v})$构成的集合，与列空间相关，$T$的核是所有满足$T({\bf v})=0$的输入向量${\bf v}$，与NULL空间相关；

任何线性变换都可以表示为矩阵与输入向量的乘积，即$T({\bf v})={\rm A}{\bf v}$。
===7.2 线性变换的矩阵形式===
**关键词**：基变换（Change of Basis）、小波变换（Wavelet Transform）

我们可以为每个线性变换$T$分配一个矩阵${\rm A}$，对输入空间${\rm V}$和输出空间${\rm W}$选择不同的基时，${\rm A}$也会不同；

我们可以用矩阵来表示一部分求导与积分，如对${\bf v}=a+bx+cx^2+dx^3$求导$T({\bf v})=b+2cx+3dx^2$可以表示为：

$\left[\begin{align} 0&& 1&& 0&& 0 \\ 0&& 0&& 2&& 0 \\ 0&& 0&& 0&& 3 \end{align}\right]$$\left[\begin{align} a\\ b\\ c\\ d\\ \end{align}\right]=$$\left[\begin{align} b\\ 2c\\ 3d\\ \end{align}\right]$；

假设$T$从空间${\rm V}$（$n$维）变换到空间${\rm W}$（$m$维），为${\rm V}$所选取的基为${\bf v}_1,\cdots,{\bf v}_n$，为${\rm W}$所选取的基为${\bf w}_1,\cdots,{\bf w}_m$，用如下方式构造${\rm A}$：
* ${\rm A}$的第$j$列通过对第$j$个基向量${\bf v}_j$应用$T$得到
* $T({\bf v}_j)=a_{1j}{\bf w}_1+\cdots+a_{mj}{\bf w}_m$

如果${\rm A}$与${\rm B}$分别代表线性变换$T$与$S$，则${\rm AB}$便是线性变换$T(S({\bf u}))$，注意一般情况下${\rm ST \neq TS}$；

基变换矩阵${\rm M}$表示的是$T({\bf v})={\bf v}$，它的列向量是输出基表示为输入基线性组合时的系数，同时${\rm M}^{-1}$可以用于坐标变换；

通过小波变换对信号进行处理后，根据需求舍弃高频或低频部分产生压缩效果，如图像压缩等。
===7.3 对角化与伪逆===
**关键词**：极分解（Polar Decomposition）、伪逆（Pseudoinverse）

对${\rm A}$进行分别可以看成选择不同的基，更好的基产生更好的矩阵，输入基矩阵放在左侧，输出基矩阵放在右侧，通过基变换可以使线性变换更简单；

对角化${\rm W}^{-1}_{{standard}\to{\bf w}}{\rm A}_{standard}{\rm W}_{{\bf w}\to{standard}}=\Lambda_{{\bf w}\to{\bf w}}$就是把输入和输出基都变换成${\rm W}$的列向量；

奇异值分解${\rm U}^{-1}_{{standard}\to{\bf u}}{\rm A}_{{standard}}{\rm V}_{{\bf v}\to {standard}}=\Sigma_{{\bf v}\to{\bf u}}$就是把输入基变换成${\rm A^TA}$的特征向量，输出基变换成${\rm AA^T}$；

对任意实数方阵${\rm A}$根据SVD且${\rm V^TV}$分解有${\rm A=U\Sigma V^T=UV^TV\Sigma V^T=(UV^T)(V\Sigma V^T)=(Q)(H)}$，其中${\rm Q}$是正交阵，${\rm H}$是对称半正定的，如果${\rm A}$可逆则${\rm H}$是对称正定的；

以上过程便是极分解，它把线性变换中旋转（在${\rm Q}$中）与伸展（在${\rm H}$中）分开：
* ${\rm H}$的特征值为${\rm A}$的奇异值，它们给出的是伸展因子
* ${\rm H}$的特征向量是${\rm A^TA}$的特征向量，它们给出的是伸展方向（轴）
* ${\rm Q}$则是旋转上述轴

对任何矩阵${\rm A_{m\times n}}$都有${\rm A=U\Sigma V^T}$，则${\rm A}$的伪逆为：

${\rm A_{n\times m}^+=V_{n\times n}\Sigma^+_{n\times m}U^T_{m\times m}}=$$\left[\begin{align} v_1&& \cdots&& v_r&& \cdots&& v_n\end{align}\right]$$\left[\begin{align} \sigma_1^{-1}&& && &&  \\ && \ddots&& &&  \\ && && \sigma_r^{-1}&& \\ && && && \end{align}\right]$$\left[\begin{align} u_1&& \cdots&& u_r&& \cdots&& u_m\end{align}\right]^{\rm T}$

它从${\rm A}$的列空间转换回到${\rm A}$的行空间；

${\rm AA^+}$把矩阵投影到${\rm A}$的列空间上，${\rm A^+A}$把矩阵投影到${\rm A}$的行空间上；

在最小二乘的过程中，当${\rm A}$的列向量线性相关时，${\rm A^TA}\hat{\bf x}={\rm A^T}{\bf b}$有很多解，其中一个解为${\bf x}^+={\rm A^+}{\bf b}$；

${\bf x}^+$在统计学中很重要，因为实验得到的${\rm A}$一般是列线性相关的。
====8 应用====
===8.1 工程中的矩阵===
**关键词**：工程（Engineering）

工程问题常常产生对称矩阵${\rm K}$，它通常是正定的，一般来源于${\rm K=A^TA}$或${\rm K=A^TCT}$，矩阵${\rm C}$往往是对角的，包含一些正的物理常数；

工程问题转变为线性代数问题有两种思路：
* 直接：物理问题由有限部分组成，把各部分链接起来的法则是线性的，这些法则可以由矩阵表示
* 间接：物理系统是“连续“的，法则通过微分方程表示，用有限微分方程来近似精确解

现代工程中不涉及到矩阵几乎是不可能的，而在应用中总会发现矩阵性质与自然法则恰好吻合；

注意在微分近似中不同近似表达式可以得到不同的精度，如：

$\frac{du}{dx}\simeq \frac{u(x+\delta x)-u(x)}{\delta x}$或者$\frac{u(x)-u(x-\delta x)}{\delta x}$或者$\frac{u(x+\delta x)-u(x-\delta x)}{2\delta x}$

前两者误差为$\delta x$，最后那个的误差为$(\delta x)^2$。
===8.2 图和网络===
**关键词**：图（Graph）、关联矩阵（Incidence Matrix）、环流（Loop Current）

图关联矩阵告诉我们$m$条边与$n$个结点的连接情况，通常情况下$m>n$，关联矩阵的元素是-1、0或者1；

对关联矩阵进行消元就是把对应的图变成树（树是没有闭环的图）；

关联矩阵${\rm A}$在四个子空间中有如下性质：
* 常数向量$(c,c,\cdots,c)$构成${\rm A}$的NULL空间
* 有$r=n-1$行线性无关，来自于任何一棵树
* ${\rm A}{\bf x}$的元素相加等于0，相当于沿着一个环相加
* 对于一个结点流入等于流出，则${\rm A^T}{\bf y}=0$的解是环流，$N({\rm A^T})$维度是$m-r$，图中有$m-r=m-n+1$个独立环。
  ===8.3 马尔科夫矩阵、人口与经济===
  **关键词**：马尔科夫矩阵（Markov Matrix）、稳态（Steady State）、佩龙-弗罗宾尼斯定理（Perron-Frobenius Theorem）

如果一个矩阵${\rm A}$每个元素都是非负的且每一列的和都为1，则它是一个马尔科夫矩阵；

如果矩阵${\rm A}$是马尔科夫矩阵，则有如下性质：
* $\lambda_{max}=1$
* 如果${\bf u}_0$非负，${\bf u}_1={\rm A}{\bf u}_0$，${\bf u}_1$也非负，如果${\bf u}_0$元素相加等于1则${\bf u}_1$也是
* 根据上一条可知${\rm A}^k$也是马尔科夫矩阵

如果${\rm A}$是一个正马尔科夫矩阵（$a_{ij}>0$），则$\lambda_1=1$比其它特征值都大，与之对应的特征向量${\bf x}_1$是稳态：
* ${\bf u}_{k}={\rm A}{\bf u}_{k-1}={\rm A}^k{\bf u}_0$，${\bf u}_k={\bf x}_1+c_2(\lambda_2)^k{\bf x}_2+\cdots+c_n(\lambda_n)^k{\bf x}_n$，${\bf u}_{\infty}={\bf x}_1$

马尔科夫矩阵${\rm A}$的第二大特征值的绝对值$|\lambda_2|$控制收敛到稳态的收敛速度，非正马尔科夫矩阵中可能有$|\lambda_2|=1$，则不存在稳态；

马尔科夫矩阵的稳态与谷歌PageRank中随机游走过程有很大的关系；

${\rm A}>0$下的佩龙-弗罗宾尼斯定理是：${\rm A}{\bf x}=\lambda_{max}{\bf x}$中所有数严格为正；

经济学中经常碰见如下问题：寻找满足${\bf p}-{\rm A}{\bf p}={\bf y}$（或${\bf p}=({\rm I-A}^{-1}{\bf y})$）的非负${\bf p}$，其中${\bf y}$与${\bf A}$也是非负的，问题实际是在探索何时$({\rm I-A})^{-1}$非负，如果$\lambda_1$是${\rm A}$最大的特征值（肯定也是正的），则有：
* 如果$\lambda_1>1$，则$({\bf I-A})^{-1}$可能有负元素
* 如果$\lambda_1=1$，则$({\bf I-A})^{-1}$不存在
* 如果$\lambda_1<1$，则$({\bf I-A})^{-1}$非负

$({\rm I-A})^{-1}$的展开是几何级数：$({\rm I-A})^{-1}={\rm I+A+A^2+A^3+\cdots}$，当${\rm A}$所有特征值满足$|\lambda|<1$时收敛。
===8.4 线性规划===
**关键词**：线性规划（Linear Programming）、对偶问题（Dual Problem）、单纯形法（Simplex Method）、内点法（Interior Point Method）

假设现在有一代价向量${\bf c}$，线性规划是在满足${\rm A}{\bf x}={\bf b}$与${\bf x}\ge 0$的情况下，使得${\bf c}\cdot{\bf x}$最小；


上述问题的对偶问题是在满足${\rm A^T}{\bf y}\le{\bf c}$情况下，使得${\bf b}\cdot{\bf y}$最大，令代价${\bf c}\cdot{\bf x}^*$最小等价于${\bf b}\cdot{\bf y}^*$最大；

求解线性规划一般有单纯形法与内点法，单纯形法时沿着“壳”走——从一个角到另外一个角，而内点法是从可行域穿过，大部分软件都是用后者来求解线性规划。
===8.5 傅里叶级数===
**关键词**：傅里叶级数（Fourier Series）、希伯特空间（Hilbert Space）

当且仅当$||{\bf v}||^2={\bf v}\cdot{\bf v}=v_1^2+v_2^2+v_3^2+\cdots$有限时，向量${\bf v}=(v_1,v_2,\cdots)$在无限维希伯特空间中；

如果${\bf v}$与${\bf w}$都在无限维希伯特空间中，根据$|{\bf v}\cdot{\bf w}|\le||{\bf v}||||{\bf w}||$可知${\bf v}\cdot{\bf w}$也在无限维希伯特空间中；

我们把函数$f(x)$与$g(x)$的内积定义为$(f,g)=\int_{0}^{2\pi}{f(x)g(x)dx}$，则$||f(x)||^2=\int_0^{2\pi}{(f(x))^2dx}$，其中区间$[0,2\pi]$是根据实际需要可变的；

任意函数的傅里叶级数展开为$y(x)=a_0+a_1\cos x+b_1\sin x+a_2\cos 2x+b_2\sin 2x+\cdots$；

上式中$a_0=\frac{1}{2\pi}\int_0^{2\pi}{f(x)\cdot 1 dx}=\text{average value of }f(x)$，$a_k=\frac{1}{\pi}\int_0^{2\pi}{f(x)\cos kx dx}$，$b_k=\frac{1}{\pi}\int_0^{2\pi}{f(x)\sin kx dx}$；

函数$1,\cos x,\sin x,\cos 2x,\sin 2x,\cdots$是希伯特空间的一组基，这组基是正交的；

可见当基互相正交时，系数（或者可以直观的认为是坐标值）可以很容易求得。
===8.6 统计与概率中的矩阵===
**关键词**：校正矩阵（Correct Matrix）、均值（Mean）、方差（Variance）、协方差矩阵（Covariance Matrix）、主成分分析（Principal Component Analysis）

${\rm A}{\bf x}={\bf b}$中$b_1,\cdots,b_m$地位等同，我们现在为之添加权重$w_1,\cdots,w_m$，放在对角矩阵${\rm W}$的对角线上：
* ${\rm A}{\bf x}={\bf b}$变成${\rm WA}{\bf x}={\rm W}{\bf b}$，可解性没有发生任何变化
* 最小二乘${\rm A^TA}\hat{{\bf x}}={\rm A^T}{\bf b}$对应变成${\rm (WA)^TWA}\hat{{\bf x}}={\rm (WA)^TW}{\bf b}$，即${\rm A^TCA}\hat{{\bf x}}={\rm A^TC}{\bf b}$，其中${\rm C=W^TW}$
* 统计学上称${\rm C}$为校正矩阵，如果协方差矩阵为$\Sigma$，则${\rm C=\Sigma^{-1}}$；

取值$b_1,\cdots,b_m$的概率分别为$p_1,\cdots,p_m$，则均值$m$与方差$\sigma^2$分别为：
* $m={\rm E}[b]=\sum{b_jp_j}$
* $\sigma^2={\rm E}[(b-m)^2]=\sum{(b_j-m)^2p_j}$；

协方差矩阵$\Sigma$中元素$\sigma_{ij}={\rm E}[(b_i-m_i)(b_j-m_j)]=\sum\sum{p_{ij} (b_i-m_i)(b_j-m_j)}$，即$\Sigma={\rm E}[({\bf b}-{\rm E}[{\bf b}])({\bf b}-{\rm E}[{\bf b}])]$；

主成分分析目的是寻找数据矩阵${\rm A}$中有用的信息，也就是寻找那些能提供最多信息的行或列，而我们认为方差大的行或列提供的信息要多于方差小的行或列；

进行主成分分析有很多方法，比如对矩阵进行奇异值分解后保留与大奇异值所对应的特征向量。
===8.7 计算机图形学===
**关键词**：同质坐标（Homogeneous Coordinate）、变换矩阵（Translation Matrix）、缩放矩阵（Scaling Matrix）、镜像矩阵（Mirror Matrix）

变换三维空间需要$4\times 4$的矩阵，因为没有$3\times 3$的矩阵可以实现移动原点，引入同质坐标：$(x,y,z)$的同质坐标为$(x,y,z,1)$；

计算机图形学中使用的是行向量，则进行变换时需要行乘矩阵而不是矩阵乘列；

变换矩阵把三维空间沿着${\bf v}_0=(v_1,v_2,v_3)$平移，
$T=\left[\begin{align} 1&& 0&& 0&& 0 \\ 0&& 1&& 0&& 0 \\ 0&& 0&& 1&& 0 \\ x_0&& y_0&& z_0&& 1 \end{align}\right]$；

$x,y,z$三个方向缩放因子为$c_1,c_2,c_3$的缩放矩阵为
$S=\left[\begin{align} c_1&& 0&& 0&& 0 \\ 0&& c_2&& 0&& 0 \\ 0&& 0&& c_3&& 0 \\ 0&& 0&& 0&& 1 \end{align}\right]$；

在二维空间中将${\bf v}=(x,y)$绕点$(a,b)$旋转$\theta$的过程为

${\bf v}{\rm T_{-}RT_{+}}=$$\left[\begin{align} x&& y&& 1 \\\end{align}\right]$$\left[\begin{align} 1&& 0&& 0 \\ 0&& 1&& 0 \\ -a&& -b&& 1 \end{align}\right]$$\left[\begin{align} \cos\theta&& -\sin\theta&& 0 \\ \sin\theta&& \cos\theta&& 0 \\ 0&& 0&& 1\end{align}\right]$$\left[\begin{align} 1&& 0&& 0 \\ 0&& 1&& 0 \\ a&& b&& 1 \end{align}\right]$；

在三维空间中要以${\bf a}=(a_1,a_2,a_3)$为轴旋转$\theta$，旋转矩阵${\rm R}$为
${\rm R}=\left[\begin{align} && && &&0 \\ && {\rm Q}&& &&0\\ && && &&0  \\ 0&& 0&& 0&& 1 \end{align}\right]$，

其中
${\rm Q}=(\cos\theta){\rm I}+(1-\cos\theta)$$\left[\begin{align} a_1^2&& a_1a_2&& a_1a_3 \\ a_1a_2&& a_2^2&& a_2a_3 \\ a_1a_3&& a_2a_3&& a_3^2\end{align}\right]$$-\sin\theta$$\left[\begin{align} 0&& a_3&& -a_2 \\ -a_3&& 0&& a_1 \\ a_2&& -a_1&& 0 \end{align}\right]$；

投影到法线为${\bf n}$且过原点的平面上所使用的投影矩阵为
${\rm P}=\left[\begin{align} && && &&0 \\ && {\rm I}-{\bf n}{\bf n}^{\rm T}&& &&0\\ && && &&0  \\ 0&& 0&& 0&& 1 \end{align}\right]$

如果投影到的平面不过原点，则先利用变换矩阵把平面上一点${\bf v}_0$平移到原点

${\rm T_{-}PT_{+}}=$$\left[\begin{align} {\rm I}&& 0 \\ -{\bf v}_0&& 1\\\end{align}\right]$$\left[\begin{align} {\rm I}-{\bf n}{\bf n}^{\rm T}&& 0 \\ 0&& 1\\\end{align}\right]$$\left[\begin{align} {\rm I}&& 0 \\ {\bf v}_0&& 1\\\end{align}\right]$；

把上述投影矩阵中的${\rm I}-{\bf n}{\bf n}^{\rm T}$改为${\rm I}-2{\bf n}{\bf n}^{\rm T}$得到的便是镜像矩阵，其他分析过程均相同。

====9 数值线性代数====
===9.1 实践中的高斯消元===
**关键词**：数值线性代数（Numerical Linear Algebra）、舍入误差（Roundoff Error）、部分主元法（Partial Pivoting）

数值线性代数需要在“快速”解与“精确”解之间掌握好平衡，精确解需要我们避免不必要的大数值与小数值；

矩阵计算量分为三个层次：
- 向量线性组合$a{\bf u}+{\bf v}$需要$O(n)$的工作，比如单步消元工作
- 矩阵乘向量${\rm A}{\bf u}+{\bf v}$需要$O(n^2)$的工作，比如对整列进行消元
- 矩阵乘矩阵${\rm AB+C}$需要$O(n^3)$的工作

计算机表示的精度有限，存在舍入误差，这导致小的主元在实践中非常危险；

解决上述问题的一个直观想法是每此都选当前行开始往下这列中最大的数作为主元，把选中的行与当前行互换，这被称为部分主元法；

对于正定矩阵行交换是没有必要的，部分主元法不会带来任何提升；

计算机实际上就是使用这种带有部分主元法的消元法来求解${\rm A}{\bf x}={\bf b}$的；

如果${\rm A}$是带状矩阵，则LU分解得到的${\rm L}$与${\rm U}$也是带状的，但${\rm A}^{-1}$却所有元素均不为零；

从2.5节中我们可知带状矩阵的消元更快，所以我们希望对稀疏矩阵进行“重排”，让非零元素都集中到“带”中，等同于LU分解后${\rm L}$与${\rm U}$中非零元素尽可能少：
* 求绝对最少是NP困难问题，代价太大
* amd（approximate minimum degree）是一个不错的近似算法，它是基于贪心的

通过正交化我们得到QR分解${\rm A=QR}$，前面我们看到可以用Gram-Schmidt得到QR分解，但还有两个更好的（Givens和Householder）；

Givens是基于旋转矩阵的，Householder基于反射矩阵，Householder比Givens快且稳定。

===9.2 范数与条件数===
**关键词**：范数（Norm）、条件数（Condition Number）

矩阵范数$||{\rm A}||$要满足如下两个条件：$||{\rm A+B}||\le ||{\rm A}||+||{\rm B}||$与$||c{\rm A}||=|c|||{\rm A}||$；

根据$||{\rm A}{\bf x}||\le ||{\rm A}||||{\bf x}||$与$||{\rm AB}||\le ||{\rm A}||||{\rm B}||$，范数$||{\rm A}||$可以控制${\bf x}\to {\rm A}{\bf x}$与${\rm A}\to{\rm AB}$的速度；

根据上条我们可以很自然地把矩阵范数定义为$||{\rm A}||=\max\limits_{x\neq 0}\frac{||{\rm A}{\bf x}||}{||{\bf x}||}$，这并不好解，做如下转化：
* 根据$||{\rm A}||^2=\max\limits_{x\neq 0}\frac{||{\rm A}{\bf x}||^2}{||{\bf x}||^2}= \max\limits_{x\neq 0}\frac{{\bf x}^{\rm T}{\rm A^TA}{\bf x}}{{\bf x}^{\rm T}{\bf x}}=\lambda_{\max}({\rm A^TA})$可知，$||{\rm A}||=\sqrt{\lambda_{\max}({\rm A^TA})}$
* 也就是说$||{\rm A}||$等于${\rm A}$的最大奇异值$\sigma_{\max}$；

条件数用来衡量方程对误差的敏感程度，它是$c=||{\rm A}||||{\rm A}^{-1}||=\frac{\lambda_{\max}}{\lambda_{\min}}$，可见条件数最小为1，它表示存在于问题内部不可避免的不稳定性；

误差可能来源于${\Delta {\rm A}}$或者${\Delta {\bf b}}$，条件数对两种误差的敏感程度都可以很好地衡量；

一条有用的经验法则是：计算机可以在舍入过程丢掉十进制小数点从第$\log c$开始后面的数据；

假设${\bf x}^*$是${\rm A}{\bf x}={\bf b}$的实际解（可能存在误差），在实际问题中：
* 有时我们希望$\Delta {\bf x}=|{\bf x}^*-{\bf x}|$尽可能小
* 有时我们希望$|{\bf b}-{\rm A}{\bf x}^*|$尽肯能小
* 在某些情况下两种可能相矛盾。
  ===9.3 迭代方法与预调节器===
  **关键词**：迭代（Iterative）、预调节器（Preconditioner）、谱半径（Spectral Radius）

对于${\rm A}{\bf x}={\bf b}$，我们可以把${\rm A}$分成${\rm S-T}$后改写成${\rm S}{\bf x}={\rm T}{\bf x}+{\bf b}$；

通过${\rm S}{\bf x}_{k+1}={\rm S}{\bf x}_{k}+{\bf b}$从${\bf x}_{k}$到${\bf x}_{k+1}$不断迭代间接求解${\rm A}{\bf x}={\bf b}$：
* 当${\bf x}_{k}$与${\bf x}_{k+1}$足够接近时停止
* 或${\bf r}_k={\bf b}-{\rm A}{\bf x}_k$足够接近零是停止

我们要非常小心地分解${\rm A}$，要在每次迭代的速度与收敛速度（关系到迭代所需次数）取得良好平衡：
* 每次迭代次数由${\rm S}$决定，比如预调节器${\rm S}$可是取${\rm A}$的对角线
* 收敛速度由${\rm B=S^{-1}T}$决定，谱半径$\rho ({\rm B})=\max|\lambda({\rm B})|$，且$\rho <1$时才收敛

常见的分解${\rm A}$的方法有：
* Jacobi's method(J)：${\rm S}$是${\rm A}$的对角线
* Gauss-Seidel method(GS)：${\rm S}$包含${\rm A}$的下三角及对角线，根据$\rho_{GS}=(\rho_J)^2$GS收敛比J快两倍
* Successive Overrelaxation(SOR)：上面两种方法的混合，需要引入参数$\omega$，但收敛更快
* Incomplete LU method(ILU)：把LU分解中${\rm L}$与${\rm U}$中小的非零元素设为零后为${\rm L}_0$与${\rm U}_0$，令${\rm S=L_0U_0}$

特征值也可以通过迭代的方式求解，常用的有Power Method与QR Method，软件使用后者来求解特征值；

$|\lambda-a_{ii}|\le r_i$，其中$r_i=\sum_{j\neq i}|a_{ij}|$，通过它可以粗略估计特征值范围，进一步可以判定可逆或者正定等。
====10 复向量与复矩阵====
===10.1 复数===
**关键词**：复数（Complex Number）、复平面（Complex Plane）、极坐标形式（Polar Form）

我们假设$i^2=-1$（电气工程中使用$j$），则$a+bi$是复数域${\rm C}^n$中的一个数，$a$是实部，$b$是虚部：
* $(a+bi)+(c+di)=(a+c)+(b+d)i$
* $(a+bi)(c+di)=(ac-bd)+(ad+bc)i$
* $z=a+bi$的共轭为$\bar{z}=a-bi$；

如果${\rm A}{\bf x}=\lambda{\bf x}$且${\rm A}$是实矩阵，则有${\rm A}\bar{{\bf x}}=\bar{\lambda}\bar{{\bf x}}$；

复平面以实部为$x$轴，以虚部为$y$轴，$a+bi$以$(a,b)$为坐标值；

通过如下方式把复数$z=a+bi$表示成极坐标形式：
* $r=|z|=\sqrt{a^2+b^2}$
* $a=r\cos\theta,b=r\sin\theta$
* $z=r\cos\theta+ir\sin\theta=re^{i\theta}$；

在复数的极坐标下，有如下一些性质，其中$z=a+bi$：
* $\bar{z}=re^{i(2\pi-\theta)}=re^{-i\theta}$ 
* $z^n=r^n(\cos n\theta+i\sin n\theta)$
* $z^n=1$的解为$1,w,w^2,\cdots,w^{n-1}$，其中$w=e^{2\pi i/n}$。
  ===10.2 哈密特与酋矩阵===
  **关键词**：哈密特矩阵（Hermitian Matrix）、酋矩阵（Unitary Matrix）

在复数空间中，当你把向量${\bf z}$或矩阵${\rm A}$转置的同时也要取共轭，被称为哈密特，记为${\rm H}$，如${\rm A}$的哈密特为$\bar{\rm A}^{\rm T}={\rm A}^{\rm H}$，有时也被记为${\rm A}^{*}$；

关于共轭转置与普通转置都有类似性质：$({\rm AB})^{\rm H}={\rm B}^{\rm H}{\rm A}^{\rm H}$；

如果一个矩阵满足${\rm A=A^H}$，我们称之为哈密特矩阵，它有如下性质：
* 对任意向量${\bf z}$，${\bf z}^{\rm H}{\rm A}{\bf z}$是个实数
* 每个特征值都是实数，特征向量相互正交（当对应不同特征值时）
* ${\rm A^HA}$是哈密特矩阵，如果${\rm A}$的列线性无关，则${\rm A^HA}$正定；

如果${\rm U}$列标准正交化，则有${\rm U^HU=I}$，如果它是方阵，它是个酋矩阵，酋矩阵有如下性质：
* ${\rm U^H=U^{-1}}$
* 酋矩阵特征向量满足$|\lambda|=1$
* ${\rm U}^{-1}$也是酋矩阵，如果${\rm V}$是酋矩阵，则${\rm UV}$也是酋矩阵。

===10.3 快速傅里叶===
**关键词**：快速傅里叶变换（The Fast Fourier Transform）

$n\times n$的傅里叶变换包含$w=e^{2\pi i/n}$的幂，第$j$行第$k$列是$w^{jk}$，第0行与第0列全是$w^0=1$：

${\rm F}_n{\bf c}=\left[\begin{align} 1&& 1&& 1&& \cdot && 1 \\ 1&& w&& w^2&& \cdot && w^{n-1} \\ 1&& w^2&& w^4&& \cdot && w^{2(n-1)} \\ \cdot&& \cdot&& \cdot&& \cdot && \cdot \\ 1&& w^{n-1}&& w^{2(n-1)}&& \cdot && w^{(n-1)^2} \end{align}\right]$$\left[\begin{align}c_0\\c_1\\c_2\\\cdot\\c_{n-1}\end{align}\right]=$$\left[\begin{align}y_0\\y_1\\y_2\\\cdot\\y_{n-1}\end{align}\right]={\bf y}$

${\bf c}$是“频率空间”的坐标值，${\bf y}$是“物理空间”的坐标值；

快速傅里叶变换是把${\rm F}_n$与${\rm F}_{n/2}$联系在一起，以${\rm F}_{1024}$到${\rm F}_{512}$为例：

${\rm F}_{1024}=\left[\begin{align}{\rm I}_{512}&& {\rm D}_{512}\\{\rm I}_{512}&& -{\rm D}_{512}\end{align}\right]$$\left[\begin{align}{\rm F}_{512}&& \\&& {\rm F}_{512}\end{align}\right]$$\left[\begin{align}\text{even-odd}\\\text{permutation}\end{align}\right]$

其中${\rm I}_{512}$是单位矩阵，${\rm D}_{512}$是对角元素为$(1,w,\cdots,w^{511})$的对角矩阵，置换矩阵把输入向量${\bf c}$分为偶数部分${\bf c}'=(c_0,c_2,\cdots,c_{1022})$与奇数部分${\bf c}''=(c_1,c_3,\cdots,c_{1023})$；

设$m=\frac{1}{2}n$，则${\bf y}={\rm F}_n{\bf c}$被分为${\bf y}'={\rm F}_m{\bf c}',{\bf y}''={\rm F}_m{\bf c}''$：
* 得到${\bf y}$的前半部分与后半部分分别为${\rm I}{\bf y}'+{\rm D}{\bf y}''$与${\rm I}{\bf y}'-{\rm D}{\bf y}''$
* ${\bf y}$每个元素具体为$y_j=y'_j+w_n^jy''_j,y_{j+m}=y'_j-w_n^jy''_j,j=0,\cdots,m-1$
* 最后对${\bf y}$进行奇偶重排就完成了傅里叶变换；

进一步把${\rm F}_{n/2}$与${\rm F}_{n/4}$通过同样的方式联系起来并不断地递归下去便是快速傅里叶变换，时间复杂度从$O(n^2)$降到$O(n\log n)$；

根据${\rm F}_n\bar{\rm F}_n=n{\rm I}$得到${\rm F}_n^{-1}=\bar{\rm F}_n/n$，对${\rm F}_n^{-1}$应用快速傅里叶变换与${\rm F}_n$类似；

如果一个矩阵可以分解为${\rm P=F\Lambda F^{-1}}$，其中${\rm F}$是傅里叶矩阵，则可以利用快速傅里叶变换加速矩阵乘法与幂运算。